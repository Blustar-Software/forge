  
**Proposal: AI-Assisted Dynamic Challenge Generation for Forge**  
  
**1. Purpose and Scope**  
  
The purpose of this proposal is to define a new **AI Mode** for **Forge** that dynamically generates programming and reasoning challenges within clearly defined constraints using an embedded reasoning-capable language model (e.g., **Phi-4 Mini / Phi-4 Mini Reasoning**). This mode aims to expand Forge’s capability to produce high-quality, constraint-adherent, and pedagogically structured challenges without manual crafting for every instance.  
  
This AI Mode will be designed for **challenge creation only** and must not replace curated content. It will operate within a verification pipeline that ensures correctness, adherence to constraint specifications, and pedagogical suitability before challenges reach learners.  
  
⸻  
  
**2. Objectives**  
  
**Primary**  
	1.	Generate **valid, constraint-conforming challenge instances**.  
	2.	Maintain **pedagogical integrity** by anchoring generation to an existing progression graph.  
	3.	Ensure **correctness and testability** via automated verification.  
	4.	Provide **metadata and classification** for each generated instance for analytics, difficulty calibration, and reporting.  
  
**Secondary**  
	1.	Support dynamic content variation to increase engagement and reduce repetition.  
	2.	Enable **difficulty calibration** based on measurable properties rather than subjective labels.  
  
⸻  
  
**3. System Architecture**  
  
**3.1 Overview Diagram**  
  
```
[Selector] ──> [LLM Generator] ──> [Verifier & Static Analyzer] ──> [Gatekeeper] ──> [Publish / Store]

```
  
**3.2 Components**  
  
**3.2.1 Selector**  
	•	Inputs: Forge progression graph, curriculum constraints.  
	•	output: high-level concept/goal tuple for generation.  
  
**3.2.2 LLM Generator**  
	•	Model: Phi-4 Mini / Phi-4 Mini Reasoning.  
	•	Produces:  
	•	Challenge prompt text.  
	•	Formal specification (JSON).  
	•	Starter code skeleton.  
	•	Reference solution.  
	•	Unit tests: public + hidden.  
	•	Metadata (concept tags, forbidden patterns).  
  
**3.2.3 Verifier & Static Analyzer**  
	•	Validates:  
	•	Tests enforce correct behavior.  
	•	No forbidden constructs/APIs in solutions.  
	•	Code compiles and runs.  
	•	Static analysis checks for structural constraints.  
	•	Executes reference and negative tests.  
  
**3.2.4 Gatekeeper**  
	•	Accepts or rejects based on verification.  
	•	On failure, triggers:  
	•	Limited automated repair loop (generator re-prompt).  
	•	Fallback to curated content if repeated failure.  
  
**3.2.5 Publishing & Storage**  
	•	Accepted challenge instances are archived with metadata.  
	•	Exposed through Forge interfaces.  
  
⸻  
  
**4. Parameter and Constraint Specification**  
  
Challenges will be driven by a **typed constraint object** rather than freeform text. Core parameters include:  
  
**Parameter**	**Description**  
concepts	Linked Forge concepts required in the challenge.  
forbiddenTokens	Banned tokens or APIs (to prevent trivializing solutions).  
difficultyRange	Numerical target range derived from measurable attributes.  
maxLines	Maximum allowed lines in reference solution.  
publicTests	Minimum count and structural coverage of public tests.  
hiddenTests	Reserved tests for validation and learner assessment.  
performanceTargets	Allowed performance constraints (e.g., time/space).  
  
Example of JSON constraint object:  
  
```
{
  "concepts": ["recursion", "memoization"],
  "forbiddenTokens": ["for", "while"],
  "difficultyRange": [6, 8],
  "maxLines": 120,
  "publicTests": 5,
  "hiddenTests": 10
}

```
  
  
⸻  
  
**5. Challenge Template Output Structure**  
  
Each generated challenge instance must contain:  
	1.	**Human-readable prompt**  
	2.	**Formal specification (JSON)**  
	3.	**Starter code skeleton**  
	4.	**Reference solution**  
	5.	**Public unit tests**  
	6.	**Hidden unit tests**  
	7.	**Metadata**  
	•	Concept tags  
	•	Constraint summary  
	•	Estimated difficulty score  
	•	Generation token trace (for traceability)  
  
⸻  
  
**6. Verification Strategy**  
  
**6.1 Functional Correctness**  
	•	Run all provided tests against the reference solution.  
	•	Include **negative test injection** (simple incorrect variants) to assess test suite strength.  
  
**6.2 Static Constraint Verification**  
	•	AST or regex based detection of banned constructs.  
	•	Structural checks (e.g., loop absence when forbidden).  
  
**6.3 Difficulty Calibration**  
	•	Measurable metrics applied:  
	•	Cyclomatic complexity  
	•	Number of test cases and coverage distribution  
	•	Concept overlap score with progression graph  
  
A challenge instance must fall within target ranges before acceptance.  
  
⸻  
  
**7. Automation and Guardrails**  
	1.	**Automated Repair Loop**  
	•	Up to a configured number of regeneration attempts.  
	•	Backtracking limited to specific components (tests, prompt, solution).  
	2.	**Fallback Mechanism**  
	•	If generalization repeatedly fails, revert to curated templates.  
	3.	**Separation of Concerns**  
	•	Learner sees only starter code and public tests.  
	•	Hidden tests and reference solutions are withheld until scoring.  
  
⸻  
  
**8. Evaluation Metrics**  
  
Post-deployment evaluation should measure:  
  
**Metric**	**Goal**  
Acceptance rate	≥ 80% under constraints  
Test coverage adequacy	≥ 90% (based on mutation-based checks)  
Learner solution convergence	Matches curated baseline  
Variation diversity	Low repetition across instances  
  
  
⸻  
  
**9. Risks and Mitigations**  
  
**Risk**	**Mitigation**  
Incoherent generations	Strict constraint objects + deterministic post-processing  
Low test quality	Automated verification + mutation injection  
Curriculum drift	Selector anchored to progression graph  
Model hallucination	Post-generation verification and gating  
  
  
⸻  
  
**10. Milestones and Roadmap**  
  
**Phase**	**Milestone**	**Deliverable**  
I	Constraint object and generator interface	JSON schema + LLM endpoint  
II	Verifier integration	Automated code/test validation harness  
III	Gatekeeper + fallback logic	Publish pipeline  
IV	Pilot deployment	Internal beta challenge pool  
V	Learner rollout	Full AI Mode in Forge  
  
  
⸻  
  
**11. Budget and Resource Estimate**  
	•	LLM inference infrastructure  
	•	Storage for challenge artifacts  
	•	Engineering time for integration (generator, verifier, analytics)  
  
Detailed cost modeling to be provided after initial approval.  
