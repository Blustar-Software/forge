
*A Link‑Inspired Cognitive Skill Development Framework for Swift Programming*

---

# **1. Purpose**

Forge is a controlled, simulation‑based training environment designed to develop **operational fluency** in Swift programming. The system emphasizes **skill isolation**, **progressive complexity**, **constraint‑driven learning**, and **adaptive feedback**, modeled after the pedagogical principles of the Link Aviation Trainer.

Forge is not a tutorial system, IDE, or problem repository. It is a **training cockpit** for Swift cognition.

---

# **2. System Objectives**

Forge must:

1. Develop deep, automatic, transferable Swift programming skills.  
2. Provide a structured, repeatable training environment.  
3. Use constraints and controlled scenarios to shape correct technique.  
4. Adapt to learner performance to maintain optimal challenge.  
5. Provide immediate, contextual feedback.  
6. Track performance longitudinally to support mastery learning.

---

# **2.1 Definitions**
**Unit**: A single, focused training exercise (challenge or project) with a clear objective and success criteria.  
**Stage**: A contiguous sequence of units grouped by syllabus intent (e.g., Core 1, Mantle 2).  
**Scenario**: The narrative wrapper and constraints around a unit’s objective.  
**Mastery**: Demonstrated stability in correctness, speed, and technique under the defined constraints.  
**Constraint**: A deliberate restriction on tools, syntax, inputs, or environment to shape correct technique.

---

# **3. Core Attributes**

## **3.0 Phased Applicability**
Forge will evolve in phases. The following attributes are **Core Requirements (Phase 1)** unless marked as **Phase 2+**. Phase 2+ items are design targets that will be implemented as the training engine matures.

## **3.1 Skill Isolation**
**FTS‑1.1** Each training unit shall target a single cognitive or technical skill.  
**FTS‑1.2** No unit shall introduce secondary concepts until the primary skill is demonstrated.  
**FTS‑1.3** Multi‑skill scenarios shall only be unlocked after prerequisite units are mastered.

---

## **3.2 Controlled Environment (Phase 2+)**
**FTS‑2.1** All exercises shall execute within a constrained sandbox defined by the system.  
**FTS‑2.2** The sandbox shall restrict available APIs, syntax, or tools as required by the training objective.  
**FTS‑2.3** The environment shall prevent unintended escape into general‑purpose Swift execution.

---

## **3.3 Repeatability**
**FTS‑3.1** All training units shall be repeatable with identical initial conditions.  
**FTS‑3.2** The system shall preserve deterministic behavior across repetitions within a given difficulty configuration.  
**FTS‑3.3** The learner shall be able to repeat any unit without penalty.

---

## **3.4 Safe Failure**
**FTS‑4.1** Errors shall be non‑destructive and contained within the sandbox.  
**FTS‑4.2** Failure shall be treated as diagnostic, not evaluative.  
**FTS‑4.3** The system shall prevent learner frustration by ensuring recoverability from all error states.

---

## **3.5 Immediate, Contextual Feedback (Phase 2+)**
**FTS‑5.1** Feedback shall be delivered quickly enough to feel immediate for the current runtime environment.  
**FTS‑5.2** Feedback shall reference the specific action or code fragment that triggered it.  
**FTS‑5.3** Feedback shall include both *what* failed and *why* it failed.

**Phase 1 Baseline (Feedback)**  
The system shall provide immediate pass/fail output and show expected vs actual results where applicable.

---

## **3.6 Progressive Difficulty**
**FTS‑6.1** Difficulty shall increase only after mastery criteria are met.  
**FTS‑6.2** Each difficulty level shall build directly on the previous one.  
**FTS‑6.3** The system shall not introduce unrelated complexity at any stage.

---

## **3.7 Scenario‑Based Learning**
**FTS‑7.1** Each unit shall define a clear objective, constraints, and success criteria.  
**FTS‑7.2** Scenarios shall simulate real cognitive demands without requiring real project context.  
**FTS‑7.3** Scenarios shall be structured, not open‑ended.  
**FTS‑7.4** Scenarios may use narrative wrappers, but the objective must remain single‑skill.

---

## **3.8 Adaptive Challenge (Phase 2+)**
**FTS‑8.1** The system shall adjust difficulty based on learner performance metrics.  
**FTS‑8.2** Adaptation shall include scaffolding, challenge escalation, or turbulence (edge cases).  
**FTS‑8.3** Adaptation shall occur without explicit learner request.

---

## **3.9 Constraint‑Driven Mastery (Phase 2+)**
**FTS‑9.1** Constraints shall be used to enforce correct technique.  
**FTS‑9.2** Constraints may include:  
- restricted syntax  
- limited APIs  
- disabled autocomplete  
- forced explicitness  
**FTS‑9.3** Constraints shall be gradually relaxed as mastery increases.

---

## **3.10 Embodied Cognitive Skill Formation**
**FTS‑10.1** Training shall target automaticity, not declarative knowledge.  
**FTS‑10.2** Units shall reinforce pattern recognition and reflexive responses.  
**FTS‑10.3** The system shall prioritize cognitive load shaping over content coverage.

---

## **3.11 Instructor‑Like Guidance**
**FTS‑11.1** The system shall monitor learner performance continuously.  
**FTS‑11.2** Guidance shall be corrective, directive, and structured.  
**FTS‑11.3** Guidance shall maintain psychological safety and avoid punitive framing.

---

## **3.12 Syllabus‑Driven Progression**
**FTS‑12.1** The system shall define a hierarchical training syllabus.  
**FTS‑12.2** Each stage shall include entry criteria, objectives, and exit criteria.  
**FTS‑12.3** Learners shall not bypass stages without demonstrated competence.

---

## **3.13 Modular Architecture**
**FTS‑13.1** Training units shall be modular and independently loadable.  
**FTS‑13.2** Modules shall compose into higher‑order skills.  
**FTS‑13.3** New modules shall be integrable without altering existing progression.

---

## **3.14 Cognitive Fidelity**
**FTS‑14.1** The system shall simulate the cognitive demands of Swift programming, not full project realism.  
**FTS‑14.2** Fidelity shall prioritize mental models, not environmental accuracy.  
**FTS‑14.3** Real‑world complexity shall be introduced only when pedagogically justified.

---

## **3.15 Performance Logging and Debriefing (Phase 2+)**
**FTS‑15.1** The system shall log attempts, errors, timings, and behavioral patterns.  
**FTS‑15.2** Debriefing shall summarize performance relative to objectives.  
**FTS‑15.3** Debriefing shall include recommendations for next steps.

---

# **3.16 Phase 1 Baseline Requirements**
**FTS‑P1.1** Units shall be deterministic and repeatable.  
**FTS‑P1.2** The system shall provide immediate pass/fail validation with expected vs actual output when applicable.  
**FTS‑P1.3** The syllabus shall gate progression at the stage level.  
**FTS‑P1.4** Adaptive selection is opt-in; the default flow remains deterministic without adaptive reshuffling.  

---

# **4. System Components**

## **4.1 Training Engine**
Executes units, enforces constraints, and evaluates learner actions.

## **4.2 Feedback Engine**
Generates contextual, real‑time feedback.

## **4.3 Adaptation Engine**
Adjusts difficulty and scaffolding based on performance.

## **4.4 Syllabus Manager**
Controls progression, prerequisites, and module sequencing.

## **4.5 Telemetry Layer**
Captures performance metrics and supports debriefing.

---

# **5. Mastery Criteria**

A learner is considered to have mastered a unit when:

1. They complete the unit successfully **N consecutive times** (N configurable).  
2. Their error rate falls below a defined threshold.  
3. Their completion time stabilizes within a target range.  
4. They demonstrate correct technique under constraint.

**Phase note:** Full mastery gating applies starting in Phase 2+. Phase 1 may gate only at the stage level using completion checks.

---

# **6. Non‑Goals**

Forge shall not:

- Serve as a general Swift IDE  
- Provide open‑ended project environments  
- Replace documentation or tutorials  
- Allow arbitrary code execution outside the training sandbox (Phase 2+ target)  

**Note:** Script-mode challenges may use lightweight stubs (for example, a tiny XCTest shim) to keep units deterministic and self-contained while still reinforcing the intended structure.

---

# **7. Phase 2 Roadmap (Draft)**

1. **Controlled execution harness (FTS‑2)**  
   - Build a challenge runner that enforces allowed APIs/syntax per challenge.  
   - Add per-challenge constraint profiles (allowed imports, disallowed tokens).  
   - Add a sandboxed run mode for challenges and verification.

2. **Feedback engine v1 (FTS‑5)**  
   - Expand validation to show expected vs actual diffs line-by-line.  
   - Emit targeted prompts based on common failure patterns.  
   - Add per-challenge diagnostic strings for common mistakes.

3. **Adaptive engine v2 (FTS‑8)**  
   - Make adaptive on by default, with a clear opt-out.  
   - Add scaffolding steps (retry variants, reduced constraints, guided retries).  
   - Track stability (consecutive passes) before stage progression.

4. **Constraint mastery ladder (FTS‑9)**  
   - Define constraint tiers (soft warn → hard fail → relaxed).  
   - Allow constraints to relax after clean passes; tighten after repeated failures.

5. **Debriefing & telemetry (FTS‑15)**  
   - Generate a short summary after each stage gate (pass rate, weak topics, time).  
   - Provide “next steps” recommendations based on weak-topic scores.  
   - Add a `forge report` command.

6. **Curriculum alignment + tooling**  
   - Add a sequencing/constraint audit tool.  
   - Add a determinism check to ensure fixtures exist when needed.  
   - Expand tests for runner, feedback, and adaptive gating.

**Note:** When we resume Phase 2, break this roadmap into 2.1/2.2/2.3 milestones and add effort estimates per item.
**Note:** Variant challenges per topic should be scheduled after Phase 2 engine work (adaptive/constraints), or piloted in a small subset first.

---

# **8. Phase 2 Controlled Execution (Design Draft)**

## **8.1 Constraint Profiles**
Each challenge can optionally define a constraint profile that the execution harness enforces.
Proposed profile fields (add to `Challenge` when implemented):
- `allowedImports: [String]` (default empty = allow standard)
- `disallowedTokens: [String]` (exact tokens or token patterns)
- `requiredTokens: [String]` (must appear in solution)
- `allowFileIO: Bool` (default true; block file APIs if false)
- `allowNetwork: Bool` (default false)
- `allowConcurrency: Bool` (default true)
- `maxRuntimeMs: Int` (override per challenge)

## **8.2 Enforcement Strategy**
Phase 2 enforcement uses a lightweight gate:
1. Tokenize the source (existing tokenizer) and check `disallowedTokens` + `requiredTokens`.
2. Run a preflight import scan against `allowedImports`.
3. Block risky API families by token signature (file IO, network, concurrency) unless allowed.
4. Execute via the existing runner with per-challenge timeouts.

This remains heuristic (not AST-level), but is strict enough to enforce curriculum constraints without adding a full Swift parser.

## **8.3 Integration Points**
- **Main flow:** before `compileAndRun` in `validateChallenge`.
- **Stage review/random/adaptive:** before checks in `runGateChallenges` / `runPracticeChallenges`.
- **Verification:** before `compileAndRunWithTimeout` in `verifyChallengeSolutions`.

## **8.4 Telemetry**
Record constraint violations with a reason code to the performance log.

## **8.5 Future Upgrades**
- Swap token-based checks for SwiftSyntax or SourceKit-based analysis.
- Migrate from heuristic API blocking to a real sandboxed runner.

## **8.6 Assisted Solutions & Adaptive Integrity (Draft)**
To prevent solution access from bypassing adaptive mastery, solution usage should be
tracked and tied to practice. This keeps solutions available while preserving
adaptive signals.

**Design goals**
- Solutions remain accessible, but do not count as mastery.
- Solution access is visible, logged, and triggers practice.
- Practice sets are short and focused to avoid punitive feel.

**Proposed behavior**
1. **Pre‑pass solution access is allowed with confirmation** and is logged as assisted (`pass_assisted`).
2. **Viewing a solution before a pass queues a short practice set immediately after the challenge completes (and resumes after restart if interrupted).**
3. **Assisted passes do not satisfy stability requirements** for progression.
4. **Post‑pass solution access is penalty‑free** (no practice debt, no assisted flag).
5. **Optional opt‑out flag** (e.g., `--allow-early-solutions`) can disable the practice trigger.

**Telemetry changes**
- Add `pass_assisted` to adaptive stats.
- Add a performance-log event for `solution_viewed` with `mode` and `assisted` fields.
- Add a performance-log event for `practice_queued` when a solution triggers practice.

---

# **9. Phase 2 Milestones (Draft)**

## **9.1 Foundations (2–4 days)**
- Add `ConstraintProfile` to `Challenge` with basic allow/deny fields.
- Implement the pre‑run gate and wire it into main flow, stage review, random, and verification.
- Add minimal telemetry for constraint violations.

## **9.2 Feedback + Adaptive (3–6 days)**
- Improve output mismatch reporting (line diffs + expected/actual summaries). ✅
- Add common‑mistake diagnostics per challenge or topic. ✅
- Make adaptive gating more automatic (with opt‑out), add stability checks. ✅ (opt‑in adaptive with stability + cooldown)
- Add solution‑triggered practice and assisted pass tracking. ⬜️

## **9.3 Mastery + Reporting (3–5 days)**
- Add constraint mastery ladder (warn → block → relax). ✅
- Stage review debriefs + `forge report`. ✅
- Add audit tooling for sequencing/fixtures/constraints. ✅

**Note:** Estimates are rough and depend on how strict the enforcement should be.
- Prioritize realism over cognitive fidelity  

---

# **7. Compliance**

A Forge implementation is considered Link‑faithful at **Phase 1** if it satisfies all Phase 1 requirements. Full compliance (including Phase 2+ requirements) is a target for later phases.

**Implementation status:** Phase 1 implementation is complete for the current scope; remaining Phase 1 gaps are tracked below.

---

# **8. Phase 1 Gap Analysis (Current Implementation)**
This section documents known deviations between the current Forge implementation and the Phase 1 requirements.

## **Gaps**
- **Integration coverage requires ongoing verification**: Integration units are tagged today; treat this as a checklist item to re‑audit after curriculum edits (FTS‑1.1–1.3).
- **Manual checks still reduce determinism**: File I/O and external steps can vary by environment, though more units now provide fixtures or scripted inputs (FTS‑3.1/3.2).
- **Constraint enforcement is heuristic**: Early‑concept checks use per‑unit introductions and a lightweight lexical pass (tokenization after stripping comments and string literals, including multiline/raw strings, with refined operator handling and DI/mock heuristics). Checks remain heuristic and can be bypassed with `--allow-early-concepts`, and DI/mock heuristics can be disabled with `--disable-di-mock-heuristics` (FTS‑1.2).

## **Strengths**
- **Stage review gating**: Each stage includes a review gate with adaptive topic weighting before the project.
- **Deterministic defaults** for most units outside adaptive and random modes.
- **Structured scenarios** with clear objectives and success criteria.
- **Explicit syllabus and stages** that map directly to the curriculum.
- **Stage review summaries**: Pass counts and elapsed time are recorded for each stage.
- **Performance logging stub**: Attempts and timing are recorded to a local performance log.
- **Constraint enforcement default**: Early‑concept usage is blocked by default with an opt‑out flag for warnings only.
- **Integration prereq tags**: Integration challenges declare required concepts, and the engine blocks misordered prerequisites.
- **SwiftPM integration prereqs**: SwiftPM basics/dependencies/build config integration is explicitly tagged.
- **Sequencing scans are clean**: Core, Mantle, and Crust reviews report no early‑concept usage after detector refinements.
- **Fixture-backed manual checks**: Manual steps increasingly use fixtures or scripted inputs to reduce variance.
- **Adaptive stats stub**: Per-topic attempt counts are recorded to support future adaptive progression.

---

# **8.1 Stage Review Pool Rubric (Applied)**

## **Purpose**
Stage reviews should verify core mastery without wasting time on trivial items.

## **Rubric**
- **Coverage**: Include representative challenges across the stage’s core topics.
- **Synthesis**: Include at least one multi‑concept task where available.
- **Error‑prone skills**: Include 1–2 items known to cause slips (logic, indexing, optionals, etc.).
- **Avoid trivia**: Exclude “just print” or purely rote syntax once past the opening lessons.
- **Deterministic by default**: Deterministic selection from a curated pool unless adaptive is enabled.

## **Curated Pools (current)**
These lists back stage reviews and are used for deterministic selection.

- **Core 1**: 5, 8, 9, 10, 12, 13, 14, 15, 16, 18, 19, 20  
- **Core 2**: 21, 22, 23, 24, 25, 29, 30, 33, 35, 37, 39, 41, 42  
- **Core 3**: 43, 46, 47, 48, 50, 52, 58, 62, 63, 69, 71, 76, 80  
- **Mantle 1**: 123, 125, 127, 130, 131, 132, 134  
- **Mantle 2**: 135, 137, 139, 140, 142, 144  
- **Mantle 3**: 145, 147, 149, 150, 151, 153  
- **Crust 1**: 174, 175, 176, 179, 180, 181, 185, 187, 190, 191  
- **Crust 2**: 192, 193, 194, 195, 196, 197, 198, 199, 200, 209  
- **Crust 3**: 210, 211, 213, 214, 216, 218, 219, 221, 224, 227  

## **Adaptive Note**
When adaptive is enabled, selection stays within the curated pool but weights items by topic performance plus per‑challenge recency.

---

# **9. Phase 1 Compliance Checklist**
**Status legend:** ✅ Met · ⚠️ Partial · ❌ Gap

## **FTS‑P1 Baseline**
- **FTS‑P1.1 (Deterministic & repeatable units)**: ⚠️ (adaptive gates introduce nondeterministic selection)  
- **FTS‑P1.2 (Immediate pass/fail + expected vs actual)**: ✅  
- **FTS‑P1.3 (Stage‑level gating)**: ✅  

## **Core Attributes (Phase 1 scope)**
- **FTS‑1.1/1.2 (Single‑skill isolation)**: ⚠️  
- **FTS‑1.3 (Multi‑skill after prerequisites)**: ⚠️  
- **FTS‑3.1/3.2 (Repeatability/determinism)**: ⚠️ (manual‑check units vary by environment)  
- **FTS‑3.3 (Repeat without penalty)**: ✅  
- **FTS‑4.1–4.3 (Safe failure)**: ✅  
- **FTS‑6.1–6.3 (Progressive difficulty)**: ⚠️ (integration units can jump in scope)  
- **FTS‑7.1–7.3 (Scenario structure)**: ✅  
- **FTS‑12.1–12.3 (Syllabus‑driven progression)**: ✅  
- **FTS‑13.1–13.3 (Modular units)**: ✅  
- **FTS‑14.1–14.3 (Cognitive fidelity)**: ✅  

## **Notes**
- Isolation and progressive difficulty are mostly honored, but integration challenges and early‑feature usage are the primary sources of drift.
- Manual‑check units are intentionally allowed but reduce strict determinism.
- Sequencing scans across Core/Mantle/Crust are clean, but isolation remains heuristic due to lexical detection (false negatives are possible).

## **Integration Prereq Audit (Current)**
- **Tagged**: 17, 18, 78, 144, 189, 207, 225  
- **Missing tags**: none found in the current curriculum

## **Re‑audit Checklist (Post‑Curriculum Edits)**
- Re‑run `swift run forge review 1-225` to check early‑concept warnings.
- Re‑scan integration challenges for `requires` coverage and update the audit list.

---

# **10. Adaptive Progression (Phase 2+ Draft)**

## **Goal**
Use lightweight performance signals to slow down or accelerate topic progression without breaking the core deterministic flow.

## **Current Implementation (Stub)**
- Forge records per‑topic attempt counts in `workspace/.adaptive_stats`.
- Each topic stores counts for `pass`, `fail`, `compile_fail`, and `manual_pass`.
- Current data model (per line): `topic|pass=0,fail=0,compile_fail=0,manual_pass=0`
- This data is not yet used to change progression; it is purely recorded for later use.

## **Proposed Gating Logic (Phase 2+)**
- **Drift threshold**: If a topic accumulates ≥ N fails or compile failures without a pass, require extra practice from that topic.
- **Recovery threshold**: Allow progression once the topic records M consecutive passes (or a pass/fail ratio within a set window).
- **Manual checks**: Count toward completion but weigh less than automated passes.

## **Future Hooks**
- `random` mode can draw extra challenges from under‑performing topics.
- Stage review could swap in more items from weak topics.
- Per‑topic fatigue can be reduced by mixing neutral topics when a drift threshold is hit.

## **Applied (Initial)**
- `random` mode weights challenge selection by topic performance (more fails → higher weight).
- Stage review selection weights topics using adaptive stats when available.
- `random adaptive` focuses on the weakest topics for a targeted practice set.
- `stats` command exposes per-topic adaptive counts for manual review.
- `stats --reset` clears the adaptive stats file.
- Main flow can trigger a short adaptive practice set after repeated failures in a topic.
- Adaptive gating thresholds are configurable via CLI flags.
- Adaptive gating can be disabled with a CLI flag.
